{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ2vJ7xjI1XH",
        "outputId": "a57d5d7d-dbf7-401a-b4bf-ac22a977c006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Mounted at /content/drive\n",
            "âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø´Ø¯.\n",
            "ðŸ”¢ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡: ['Temperature_C', 'Pressure_psi', 'pH', 'Salinity_ppm', 'Flow_Rate_bbl_day', 'Permeability_mD', 'Porosity_pct']\n",
            "âœ… Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.\n",
            "âœ… Ø¯Ø§Ø¯Ù‡ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒâ€ŒØ´Ø¯Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\n"
          ]
        }
      ],
      "source": [
        "# --- Ù…Ø±Ø­Ù„Ù‡ 1: Ù†ØµØ¨ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ ---\n",
        "!pip install pandas pyarrow scikit-learn joblib\n",
        "\n",
        "# --- Ù…Ø±Ø­Ù„Ù‡ 2: Ù…ØªØµÙ„ Ø´Ø¯Ù† Ø¨Ù‡ Google Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Ù…Ø±Ø­Ù„Ù‡ 3: Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ ---\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "file_path = '/content/drive/MyDrive/formation_damage_optimized.parquet'\n",
        "\n",
        "try:\n",
        "    # Ø³Ø¹ÛŒ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Parquet\n",
        "    df = pd.read_parquet(file_path)\n",
        "    print(\"âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø´Ø¯.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"âŒ ÙØ§ÛŒÙ„ Ø¯ÛŒØªØ§Ø³Øª ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø¯ÛŒØªØ§ÛŒ ØªØ³Øª Ù…ØµÙ†ÙˆØ¹ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯...\")\n",
        "    # ØªÙˆÙ„ÛŒØ¯ Ø¯ÛŒØªØ§ÛŒ ØªØ³Øª Ù…ØµÙ†ÙˆØ¹ÛŒ (Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯ ÙØ§ÛŒÙ„)\n",
        "    from generate_synthetic_data import generate_data\n",
        "    df = generate_data(n_samples=1000)\n",
        "    df.to_parquet('formation_damage_optimized.parquet')\n",
        "    print(\"âœ… Ø¯ÛŒØªØ§ÛŒ ØªØ³Øª Ù…ØµÙ†ÙˆØ¹ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Ù…Ø±Ø­Ù„Ù‡ 4: Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ ---\n",
        "numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "print(\"ðŸ”¢ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡:\", numeric_cols)\n",
        "\n",
        "# --- Ù…Ø±Ø­Ù„Ù‡ 5: Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ ---\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import joblib\n",
        "\n",
        "scalers = {}\n",
        "normalized_df = df.copy()\n",
        "\n",
        "for col in numeric_cols:\n",
        "    # ØªØ¹ÛŒÛŒÙ† Ø±ÙˆØ´ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ (Ù…Ø«Ø§Ù„: Min-Max Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ø­Ø¯ÙˆØ¯ØŒ Z-Score Ø¨Ø±Ø§ÛŒ ØªÙˆØ²ÛŒØ¹ Ù†Ø±Ù…Ø§Ù„)\n",
        "    if df[col].min() >= 0 and df[col].max() <= 1000:\n",
        "        scaler = MinMaxScaler()\n",
        "        normalized_df[col] = scaler.fit_transform(df[[col]])\n",
        "        scalers[col] = {\n",
        "            'type': 'MinMax',\n",
        "            'min': scaler.data_min_[0],\n",
        "            'max': scaler.data_max_[0]\n",
        "        }\n",
        "    else:\n",
        "        scaler = StandardScaler()\n",
        "        normalized_df[col] = scaler.fit_transform(df[[col]])\n",
        "        scalers[col] = {\n",
        "            'type': 'Standard',\n",
        "            'mean': scaler.mean_[0],\n",
        "            'std': scaler.scale_[0]\n",
        "        }\n",
        "\n",
        "# Ø°Ø®ÛŒØ±Ù‡ Ø§Ø³Ú©ÛŒÙ„Ø±Ù‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡\n",
        "joblib.dump(scalers, 'scalers_params.joblib')\n",
        "print(\"âœ… Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.\")\n",
        "\n",
        "# --- Ù…Ø±Ø­Ù„Ù‡ 6: Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒâ€ŒØ´Ø¯Ù‡ ---\n",
        "normalized_df.to_parquet('normalized_formation_damage.parquet')\n",
        "print(\"âœ… Ø¯Ø§Ø¯Ù‡ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒâ€ŒØ´Ø¯Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\")\n",
        "\n"
      ]
    }
  ]
}